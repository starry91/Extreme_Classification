{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "colab": {
      "name": "XML-AE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RitqWxRiKqLP",
        "colab_type": "text"
      },
      "source": [
        "# Extreme Classification using Ranking based AutoEncoder approach using DCCA loss\n",
        "Link to paper: https://arxiv.org/pdf/1904.05937.pdf\n",
        "\n",
        "Link to DCCA: https://ttic.uchicago.edu/~klivescu/papers/andrew_icml2013.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaqvccu_joNQ",
        "colab_type": "code",
        "outputId": "6ca31895-ed61-48ec-8494-b49e09139526",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "!pip install torchviz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\r\u001b[K     |████████                        | 10kB 18.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.4.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3523 sha256=4c357d3407d143cc1e96c22a1f52975570b61ecd0adaae1ac112e3e9ff25b07d\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP4qdctBjv4c",
        "colab_type": "code",
        "outputId": "47ce2dff-2d5b-42db-dbf0-d67030b2878c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T06:57:33.748378Z",
          "start_time": "2020-03-05T06:57:33.734155Z"
        },
        "id": "BP2Gtjp_jCht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import pandas as pd\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "from torchviz import make_dot\n",
        "from scipy.io.arff import loadarff\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(1)\n",
        "np.random.seed(1)\n",
        "# curdir = \"/content/\"\n",
        "# os.chdir('/content/drive/My Drive/xclib/')\n",
        "# sys.path.append('/content/drive/My Drive/xclib')\n",
        "# !sudo python /content/drive/My\\ Drive/xclib/setup.py install --user\n",
        "# os.chdir(curdir)\n",
        "# # print(curdir)\n",
        "# !pwd\n",
        "# import xclib.evaluation.xc_metrics as metric\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ejyznjvlPEv",
        "colab_type": "code",
        "outputId": "3aa2e954-8aa2-4356-acc4-0f5c560e6e49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "## Mediamill dataset\n",
        "!unrar x /content/drive/'My Drive'/mediamill.rar"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/drive/My Drive/mediamill.rar\n",
            "\n",
            "Extracting  mediamill.arff                                               \b\b\b\b  0%\b\b\b\b  1%\b\b\b\b  2%\b\b\b\b  3%\b\b\b\b  4%\b\b\b\b  5%\b\b\b\b  6%\b\b\b\b  7%\b\b\b\b  8%\b\b\b\b  9%\b\b\b\b 10%\b\b\b\b 11%\b\b\b\b 12%\b\b\b\b 13%\b\b\b\b 14%\b\b\b\b 15%\b\b\b\b 16%\b\b\b\b 17%\b\b\b\b 18%\b\b\b\b 19%\b\b\b\b 20%\b\b\b\b 21%\b\b\b\b 22%\b\b\b\b 23%\b\b\b\b 24%\b\b\b\b 25%\b\b\b\b 26%\b\b\b\b 27%\b\b\b\b 28%\b\b\b\b 29%\b\b\b\b 30%\b\b\b\b 31%\b\b\b\b 32%\b\b\b\b 33%\b\b\b\b 34%\b\b\b\b 35%\b\b\b\b 36%\b\b\b\b 37%\b\b\b\b 38%\b\b\b\b 39%\b\b\b\b 40%\b\b\b\b 41%\b\b\b\b 42%\b\b\b\b 43%\b\b\b\b 44%\b\b\b\b 45%\b\b\b\b 46%\b\b\b\b 47%\b\b\b\b 48%\b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Extracting  mediamill.xml                                                \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Extracting  mediamill-train.arff                                         \b\b\b\b 50%\b\b\b\b 51%\b\b\b\b 52%\b\b\b\b 53%\b\b\b\b 54%\b\b\b\b 55%\b\b\b\b 56%\b\b\b\b 57%\b\b\b\b 58%\b\b\b\b 59%\b\b\b\b 60%\b\b\b\b 61%\b\b\b\b 62%\b\b\b\b 63%\b\b\b\b 64%\b\b\b\b 65%\b\b\b\b 66%\b\b\b\b 67%\b\b\b\b 68%\b\b\b\b 69%\b\b\b\b 70%\b\b\b\b 71%\b\b\b\b 72%\b\b\b\b 73%\b\b\b\b 74%\b\b\b\b 75%\b\b\b\b 76%\b\b\b\b 77%\b\b\b\b 78%\b\b\b\b 79%\b\b\b\b 80%\b\b\b\b 81%\b\b\b\b 82%\b\b\b\b 83%\b\b\b\b 84%\b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Extracting  mediamill-test.arff                                          \b\b\b\b 85%\b\b\b\b 86%\b\b\b\b 87%\b\b\b\b 88%\b\b\b\b 89%\b\b\b\b 90%\b\b\b\b 91%\b\b\b\b 92%\b\b\b\b 93%\b\b\b\b 94%\b\b\b\b 95%\b\b\b\b 96%\b\b\b\b 97%\b\b\b\b 98%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txr7JntXf5o4",
        "colab_type": "code",
        "outputId": "06b94284-10a4-4277-ca7c-f86aa4714174",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!unzip \"/content/drive/My Drive/EURLex-4.3K.zip\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/EURLex-4.3K.zip\n",
            "   creating: EURLex-4.3K/\n",
            "  inflating: EURLex-4.3K/README      \n",
            "  inflating: EURLex-4.3K/validation.txt  \n",
            "  inflating: EURLex-4.3K/test.txt    \n",
            "  inflating: EURLex-4.3K/train.txt   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmhQzDztgXVJ",
        "colab_type": "code",
        "outputId": "e5c8090a-3619-442f-91c9-95be5274539a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!cat \"/content/EURLex-4.3K/README\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sources: \n",
            "[1] http://nlp.cs.aueb.gr/software_and_datasets/EURLEX57K/index.html\n",
            "[2] https://www.aclweb.org/anthology/W19-2209/\n",
            "[3] https://www.aclweb.org/anthology/P19-1636/\n",
            "\n",
            "Changes: Renamed to EURLex-4.3K to follow the naming convention of extreme classification repository.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA3fw1ztgY-B",
        "colab_type": "code",
        "outputId": "80f8ee65-4851-49c7-9dbc-f8c7f05986a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!ls \"/content\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive\t     mediamill.arff\t  mediamill-train.arff\tsample_data\n",
            "EURLex-4.3K  mediamill-test.arff  mediamill.xml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X06-8TBAMii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(path2):\n",
        "  path = \"/content\"\n",
        "  raw_data = loadarff(path+path2)\n",
        "  df_data = pd.DataFrame(raw_data[0])\n",
        "  X_cols = df_data.columns[:120]\n",
        "  Y_cols = df_data.columns[120:]\n",
        "  X_data = df_data[X_cols]\n",
        "  Y_data = df_data[Y_cols]\n",
        "  for col in Y_data.columns:\n",
        "    Y_data[col] = Y_data[col].apply(lambda x: x.decode(\"utf-8\"))\n",
        "    Y_data[col] = Y_data[col].astype(int)\n",
        "  return X_data.values, Y_data.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9IU_XxnAmj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = \"/mediamill-train.arff\"\n",
        "test_path = \"/mediamill-test.arff\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WClR2xZwyCB8",
        "colab_type": "code",
        "outputId": "6230959c-b9df-47f2-8cdc-92fc30f7ec76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "X_train, Y_train = get_data(train_path)\n",
        "X_test, Y_test = get_data(test_path)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T06:41:08.645519Z",
          "start_time": "2020-03-05T06:41:08.563838Z"
        },
        "id": "C-09RkXBjCjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class XML(nn.Module):\n",
        "    def calculateBowAndTfIdf(self, input):\n",
        "        return self.bow.transform(input), self.tfidf.transform(input)\n",
        "    \n",
        "    def train(self, input):\n",
        "        self.BoW = CountVectorizer(input)\n",
        "        self.TfIdf = TfidfVectorizer(input)\n",
        "    \n",
        "    def __init__(self, input_size, embedding_size, attention_layer_size,\n",
        "                 encoder_layer_size, hidden_layer_size, output_size, margin, lamda = 1, r1 = 1e-6, use_all_singular_values=False):\n",
        "        self.print_num = 0\n",
        "        self.r1 = r1      # 1e-6, 5e-7, 1e-7\n",
        "        self.m = margin\n",
        "        self.lamda = lamda\n",
        "        ########## Feature Embedding\n",
        "        ### Embedding Layer\n",
        "        nn.Module.__init__(self)\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        ### Attention Module\n",
        "        self.attentionfc1 = nn.Linear( embedding_size,  attention_layer_size)\n",
        "        self.attentionfc2 = nn.Linear(attention_layer_size, embedding_size)\n",
        "        ### fully connected layer\n",
        "        self.featurefc1 = nn.Linear(embedding_size, hidden_layer_size)\n",
        "                                      \n",
        "        ########## encoder\n",
        "        self.encoderfc1 = nn.Linear(output_size, encoder_layer_size)\n",
        "        self.encoderfc2 = nn.Linear(encoder_layer_size, hidden_layer_size)\n",
        "        \n",
        "        ########## decoder\n",
        "        self.decoderfc1 = nn.Linear(hidden_layer_size, encoder_layer_size)\n",
        "        self.decoderfc2 = nn.Linear(encoder_layer_size, output_size)\n",
        "        # self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.device = torch.device('cpu')\n",
        "        self.use_all_singular_values = use_all_singular_values\n",
        "        self.outdim_size = hidden_layer_size\n",
        "        \n",
        "    def reconstructingLoss(self, y_predicted, y_actual):\n",
        "        loss = 0\n",
        "        for i in range(y_predicted.shape[0]):\n",
        "          y_hat_n = y_predicted[i][y_actual[i]==0]\n",
        "          y_hat_p = y_predicted[i][y_actual[i]==1]\n",
        "          if(len(y_hat_p) == 0):\n",
        "            continue\n",
        "          y_p_min = torch.min(y_hat_p)\n",
        "          y_n_max = torch.max(y_hat_n)\n",
        "          loss1 = torch.sum(y_hat_n+self.m-y_p_min)\n",
        "          loss1[loss1 < 0] = 0\n",
        "          loss2 = torch.sum(-y_hat_p+self.m+y_n_max)\n",
        "          loss2[loss2 < 0] = 0\n",
        "          loss += (loss1+loss2)\n",
        "        loss /= y_predicted.shape[0]\n",
        "        # print(\"ReconstrLoss\",loss)\n",
        "        return loss\n",
        "\n",
        "    def hiddenLoss(self, H1, H2):\n",
        "        # print(\"H1 shape: \",H1.shape)\n",
        "        # print(\"H1\",H1)\n",
        "        # print(\"H2\",H2)\n",
        "        # print(H1.size())\n",
        "        r1 = self.r1\n",
        "        r2 = 1e-3\n",
        "        eps = 1e-12\n",
        "        # x = Variable(1e-4, requires_grad=False)\n",
        "        # x = torch.tensor(1e-4, requires_grad=False)\n",
        "        # x = x.fill_(1e-4)\n",
        "        # H1 = H1.clone() + x\n",
        "        # H2 = H2.clone() + x\n",
        "        H1, H2 = H1.t(), H2.t()\n",
        "        # assert torch.isnan(H1).sum().item() == 0\n",
        "        # assert torch.isnan(H2).sum().item() == 0\n",
        "\n",
        "        o1 = o2 = H1.size(0)\n",
        "\n",
        "        m = H1.size(1)\n",
        "\n",
        "        H1bar = H1 - H1.mean(dim=1).unsqueeze(dim=1)\n",
        "        # print(\"H1bar\", H1bar.shape)\n",
        "        # print(\"max\", torch.max(H1, dim=1, keepdim=True)[0].shape)\n",
        "        # H1bar /= H1.max(dim=1, keepdim=True)[0]#.unsqueeze(dim=1)\n",
        "        H2bar = H2 - H2.mean(dim=1).unsqueeze(dim=1)\n",
        "        # H2bar /= H2.max(dim=1, keepdim=True)[0]#.unsqueeze(dim=1)\n",
        "        # assert torch.isnan(H1bar).sum().item() == 0\n",
        "        # assert torch.isnan(H2bar).sum().item() == 0\n",
        "\n",
        "        SigmaHat12 = (1.0 / (m - 1)) * torch.matmul(H1bar, H2bar.t())\n",
        "        SigmaHat11 = (1.0 / (m - 1)) * torch.matmul(H1bar,\n",
        "                                                    H1bar.t()) + r1 * torch.eye(o1, device=self.device)\n",
        "        SigmaHat22 = (1.0 / (m - 1)) * torch.matmul(H2bar,\n",
        "                                                    H2bar.t()) + r2 * torch.eye(o2, device=self.device)\n",
        "        assert torch.isnan(SigmaHat11).sum().item() == 0\n",
        "        assert torch.isnan(SigmaHat12).sum().item() == 0\n",
        "        assert torch.isnan(SigmaHat22).sum().item() == 0\n",
        "\n",
        "        # Calculating the root inverse of covariance matrices by using eigen decomposition\n",
        "        # print(H1bar)\n",
        "        # print(\"Sigma11\",SigmaHat11)\n",
        "        # print(\"Sigma12\",SigmaHat12)\n",
        "        [D1, V1] = torch.symeig(SigmaHat11, eigenvectors=True)\n",
        "        [D2, V2] = torch.symeig(SigmaHat22, eigenvectors=True)\n",
        "        # assert torch.isnan(D1).sum().item() == 0\n",
        "        # assert torch.isnan(D2).sum().item() == 0\n",
        "        # assert torch.isnan(V1).sum().item() == 0\n",
        "        # assert torch.isnan(V2).sum().item() == 0\n",
        "\n",
        "        # Added to increase stability\n",
        "        # posInd1 = torch.gt(D1, eps).nonzero()[:, 0]\n",
        "        # D1 = D1[posInd1]\n",
        "        # V1 = V1[:, posInd1]\n",
        "        # posInd2 = torch.gt(D2, eps).nonzero()[:, 0]\n",
        "        # D2 = D2[posInd2]\n",
        "        # V2 = V2[:, posInd2]\n",
        "        # print(posInd1.size())\n",
        "        # print(posInd2.size())\n",
        "\n",
        "        SigmaHat11RootInv = torch.matmul(\n",
        "            torch.matmul(V1, torch.diag(D1 ** -0.5)), V1.t())\n",
        "        SigmaHat22RootInv = torch.matmul(\n",
        "            torch.matmul(V2, torch.diag(D2 ** -0.5)), V2.t())\n",
        "\n",
        "        Tval = torch.matmul(torch.matmul(SigmaHat11RootInv,\n",
        "                                         SigmaHat12), SigmaHat22RootInv)\n",
        "#         print(Tval.size())\n",
        "\n",
        "        if self.use_all_singular_values:\n",
        "            # all singular values are used to calculate the correlation\n",
        "            tmp = torch.trace(torch.matmul(Tval.t(), Tval))\n",
        "            # print(tmp)\n",
        "            corr = torch.sqrt(tmp)\n",
        "            # assert torch.isnan(corr).item() == 0\n",
        "        else:\n",
        "            # just the top self.outdim_size singular values are used\n",
        "            U, V = torch.symeig(torch.matmul(\n",
        "                Tval.t(), Tval), eigenvectors=True)\n",
        "            # print(\"U\",U)\n",
        "            U = torch.abs(U)\n",
        "            # U = U[torch.gt(U, eps).nonzero()[:, 0]]\n",
        "            U = U.topk(self.outdim_size)[0]\n",
        "            # print(\"Utop\",U)\n",
        "            # print(\"Usqrt\",torch.sqrt(U))\n",
        "            # print(\"Usum\",torch.sum(torch.sqrt(U)))\n",
        "            corr = torch.sum(torch.sqrt(U))\n",
        "        # print(\"corr\",-corr)\n",
        "        return 100-corr\n",
        "\n",
        "    def hiddenLoss1(self, x_hidden, y_hidden):\n",
        "        # print(\"xhiden shape: \",x_hidden.shape)\n",
        "        temp = torch.sum((x_hidden-y_hidden)**2)\n",
        "        temp /= x_hidden.shape[0]\n",
        "        # print(\"hiddLoss\",temp)\n",
        "        return temp\n",
        "      \n",
        "    def train(self, X_train, Y_train, lr_=0.001, epochs=10, batch_size=256):\n",
        "        X_data_new = np.array([list(range(X_train.shape[1]))]*X_train.shape[0])\n",
        "        X_TfIdftensor =  torch.from_numpy(X_train)\n",
        "        X_data_tensor =  torch.from_numpy(X_data_new)\n",
        "        Y_data_tensor =  torch.from_numpy(Y_train)\n",
        "        criterion = self.loss\n",
        "        optimizer = optim.Adam(self.parameters(),lr=lr_)\n",
        "        reg_par = 1e-5\n",
        "        # optimizer = torch.optim.RMSprop(\n",
        "        #       self.parameters(), lr=lr_, weight_decay=reg_par)\n",
        "        self.losses = []\n",
        "        # epochs = 10\n",
        "        # batch_size = 256 # or whatever\n",
        "        for i,e in enumerate(range(epochs)):\n",
        "            permutation = torch.randperm(X_data_tensor.size()[0])\n",
        "            self.print_num = 0\n",
        "            for j in range(0,X_data_tensor.size()[0], batch_size):\n",
        "                indices = permutation[j:j+batch_size]\n",
        "                optimizer.zero_grad()\n",
        "                x_hidden, y_hidden, y_predicted = self.forward(X_data_tensor[indices], X_TfIdftensor[indices][:, :, None].type('torch.FloatTensor'), Y_data_tensor[indices].type('torch.FloatTensor'))\n",
        "                loss = criterion(x_hidden, y_hidden, y_predicted, Y_data_tensor[indices].type('torch.FloatTensor'))\n",
        "                # print(\"loss = \", loss)\n",
        "                loss.backward() # Backpropagation\n",
        "                optimizer.step() # Optimizer one step\n",
        "                # print(\"#########################Gradients#########################\")\n",
        "                # for name, parameter in self.named_parameters():\n",
        "                #   print(\"Gradient \"+name, parameter.grad)\n",
        "                #   print(\"Values \"+name, parameter)\n",
        "\n",
        "            print(f\"Epoch - {i+1}, Loss - {round(loss.item(),3)}\")  # Print loss\n",
        "            self.losses.append(round(loss.item(),3))\n",
        "        plt.plot(np.array(self.losses), 'r')\n",
        "\n",
        "    def f_x(self, bow, tfidf):\n",
        "        word_embedding = self.embedding(bow)\n",
        "        #print(\"word_embedding_shape: \", word_embedding.shape)\n",
        "        #print(\"tfidf:\", tfidf.shape)\n",
        "        word_embedding = tfidf * word_embedding\n",
        "        #print(\"tfidf * word_embedding: \", word_embedding.shape)\n",
        "        attention_embedding = word_embedding\n",
        "        attention_embedding = torch.relu(self.attentionfc1(attention_embedding))\n",
        "        #print(\"Attention after fc1: \", attention_embedding.shape)\n",
        "        attention_embedding = torch.sigmoid(self.attentionfc2(attention_embedding))\n",
        "        #print(\"Attention after fc2: \", attention_embedding.shape)\n",
        "        word_embedding = attention_embedding * word_embedding\n",
        "        #print(\"Attention before pooling: \", word_embedding.shape)\n",
        "        ###word_embedding - max pooling\n",
        "        #print(\"word_embedding_shape: \", word_embedding.shape, \" \", word_embedding.type())\n",
        "        word_embedding = word_embedding.mean(1)\n",
        "        #print(\"Attention after pooling: \", word_embedding.shape)\n",
        "        x_hidden = torch.tanh(self.featurefc1(word_embedding))\n",
        "        return x_hidden\n",
        "    \n",
        "    def e_y(self, labels):\n",
        "        y_hidden = torch.relu(self.encoderfc1(labels))\n",
        "        y_hidden = torch.tanh(self.encoderfc2(y_hidden))\n",
        "        return y_hidden\n",
        "    \n",
        "    def d_y(self, y_hidden):\n",
        "        y_predicted = torch.tanh(self.decoderfc1(y_hidden))\n",
        "        y_predicted = torch.relu(self.decoderfc2(y_predicted))\n",
        "        return y_predicted\n",
        "\n",
        "    def forward(self, bow, tfidf, labels):\n",
        "        x_hidden = self.f_x(bow, tfidf)\n",
        "        y_hidden = self.e_y(labels)\n",
        "        y_predicted = self.d_y(y_hidden)\n",
        "        return x_hidden, y_hidden, y_predicted\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        X_data_new = np.array([list(range(X_test.shape[1]))]*X_test.shape[0])\n",
        "        tfidf =  torch.from_numpy(X_test)\n",
        "        tfidf = tfidf[:, :, None].type('torch.FloatTensor')\n",
        "        bow =  torch.from_numpy(X_data_new)\n",
        "        x_hidden = self.f_x(bow, tfidf)\n",
        "        y_predicted = self.d_y(x_hidden)\n",
        "        return y_predicted.cpu().detach().numpy()\n",
        "\n",
        "    def loss(self, x_hidden, y_hidden, y_predicted, y_actual):\n",
        "        loss_hidden = self.hiddenLoss(x_hidden, y_hidden)\n",
        "        loss_ae = self.reconstructingLoss(y_predicted, y_actual)\n",
        "        if self.print_num == 0:\n",
        "            print(\"Hidden loss = {0}, Reconstruction Loss = {1}\".format(loss_hidden, loss_ae))\n",
        "        self.print_num += 1\n",
        "        self.print_num %= 63\n",
        "        return (loss_hidden+self.lamda*loss_ae)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPZhhmN_zd9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def p_k(y_predicted, y_actual, k):\n",
        "  res = 0\n",
        "  for i in range(len(y_predicted)):\n",
        "    indices = y_predicted[i].argsort()[-k:][::-1]\n",
        "    res += (y_actual[i][indices].sum())/k\n",
        "  return res/len(y_predicted)\n",
        "\n",
        "def dcg_k(y_predicted, y_actual, k):\n",
        "  logger = lambda t: 1/math.log(1+t,2)\n",
        "  logger = np.vectorize(logger)\n",
        "  # l = np.arange(1,1+y_predicted.shape[1])\n",
        "  l = np.arange(1,1+k)\n",
        "  l = logger(l)\n",
        "  res = np.zeros(y_predicted.shape[0])\n",
        "  for i in range(len(y_predicted)):\n",
        "    indices = y_predicted[i].argsort()[-k:][::-1]\n",
        "    temp = l*y_actual[i][indices]\n",
        "    res[i] = temp.sum()\n",
        "    # temp = l*y_actual[i]\n",
        "    # res[i] = temp[indices].sum()\n",
        "  return res\n",
        "\n",
        "def n_k(y_predicted, y_actual, k):\n",
        "  dcg_k_list = dcg_k(y_predicted, y_actual, k)\n",
        "  logger = lambda t: 1/math.log(1+t,2)\n",
        "  logger = np.vectorize(logger)\n",
        "  res = 0\n",
        "  for i in range(len(y_predicted)):\n",
        "    lim = y_actual[i].sum()\n",
        "    if(lim == 0):\n",
        "      continue\n",
        "    l = np.arange(1,1+min(lim,k))\n",
        "    l = logger(l)\n",
        "    deno = l.sum()\n",
        "    res += dcg_k_list[i]/deno\n",
        "  return res/y_predicted.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T06:41:08.884779Z",
          "start_time": "2020-03-05T06:41:08.868401Z"
        },
        "id": "v6EAQ-DKjCjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document = \"Hey there delilah whats it like in new york city I am thousand miles away but tonight you look so pretty yes you do hey there delilah don't you worry about the distance I am right there if you get lonenly give this song another listen\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T06:41:09.417574Z",
          "start_time": "2020-03-05T06:41:09.387412Z"
        },
        "scrolled": true,
        "id": "KVoopoBAjCjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vec = CountVectorizer()\n",
        "count_occurs = count_vec.fit_transform([document])\n",
        "count_occur_df = pd.DataFrame(\n",
        "    (count, word) for word, count in\n",
        "     zip(count_occurs.toarray().tolist()[0], \n",
        "    count_vec.get_feature_names()))\n",
        "count_occur_df.columns = ['Word', 'Count']\n",
        "count_occur_df.sort_values('Count', ascending=False, inplace=True)\n",
        "print(count_occur_df.shape)\n",
        "count_occur_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T06:41:10.074598Z",
          "start_time": "2020-03-05T06:41:10.025149Z"
        },
        "id": "uvR6vVOkjCjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_vec = TfidfVectorizer()\n",
        "tfidf_count_occurs = tfidf_vec.fit_transform([document])\n",
        "tfidf_count_occur_df = pd.DataFrame(\n",
        "    (count, word) for word, count in zip(\n",
        "    tfidf_count_occurs.toarray().tolist()[0],   \n",
        "    tfidf_vec.get_feature_names()))\n",
        "tfidf_count_occur_df.columns = ['Word', 'Count']\n",
        "tfidf_count_occur_df.sort_values('Count', ascending=False, inplace=True)\n",
        "tfidf_count_occur_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNSKYQatjCkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_dot(y_hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv_9qqmujCkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_dot(y_predicted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVafYeQVjCkZ",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@titlesigmoid\n",
        "make_dot(x_hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXhceqCSrMfZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "91bbbf44-3347-42e8-f70f-9797c16a553d"
      },
      "source": [
        "model = XML(input_size=X_train.shape[1], embedding_size=100, attention_layer_size=50, \n",
        "            encoder_layer_size=120, hidden_layer_size=100,output_size=Y_train.shape[1],\n",
        "            margin=0.8, lamda = 10, r1 = 1e-6, use_all_singular_values = False)\n",
        "model.train(X_train, Y_train, lr_ = 0.01, epochs=50, batch_size=1024) "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hidden loss = 95.08831787109375, Reconstruction Loss = 79.8844985961914\n",
            "Epoch - 1, Loss - 680.157\n",
            "Hidden loss = 81.87593841552734, Reconstruction Loss = 62.50900650024414\n",
            "Epoch - 2, Loss - 619.951\n",
            "Hidden loss = 75.36859130859375, Reconstruction Loss = 54.8067512512207\n",
            "Epoch - 3, Loss - 560.185\n",
            "Hidden loss = 72.62772369384766, Reconstruction Loss = 49.09076690673828\n",
            "Epoch - 4, Loss - 591.221\n",
            "Hidden loss = 71.33250427246094, Reconstruction Loss = 51.10139846801758\n",
            "Epoch - 5, Loss - 566.95\n",
            "Hidden loss = 70.4354476928711, Reconstruction Loss = 52.07100296020508\n",
            "Epoch - 6, Loss - 563.603\n",
            "Hidden loss = 69.74983215332031, Reconstruction Loss = 54.54484176635742\n",
            "Epoch - 7, Loss - 566.195\n",
            "Hidden loss = 69.71099090576172, Reconstruction Loss = 50.370643615722656\n",
            "Epoch - 8, Loss - 556.879\n",
            "Hidden loss = 68.41996002197266, Reconstruction Loss = 53.823951721191406\n",
            "Epoch - 9, Loss - 592.871\n",
            "Hidden loss = 68.2747573852539, Reconstruction Loss = 51.962955474853516\n",
            "Epoch - 10, Loss - 576.989\n",
            "Hidden loss = 67.67939758300781, Reconstruction Loss = 51.6184196472168\n",
            "Epoch - 11, Loss - 596.321\n",
            "Hidden loss = 67.24186706542969, Reconstruction Loss = 53.81861877441406\n",
            "Epoch - 12, Loss - 590.72\n",
            "Hidden loss = 66.34249877929688, Reconstruction Loss = 50.75686264038086\n",
            "Epoch - 13, Loss - 556.421\n",
            "Hidden loss = 65.13668060302734, Reconstruction Loss = 53.967437744140625\n",
            "Epoch - 14, Loss - 573.753\n",
            "Hidden loss = 65.55290985107422, Reconstruction Loss = 52.92644119262695\n",
            "Epoch - 15, Loss - 591.699\n",
            "Hidden loss = 62.30717468261719, Reconstruction Loss = 51.841426849365234\n",
            "Epoch - 16, Loss - 578.933\n",
            "Hidden loss = 64.3310775756836, Reconstruction Loss = 52.32854461669922\n",
            "Epoch - 17, Loss - 591.346\n",
            "Hidden loss = 61.92736053466797, Reconstruction Loss = 51.43983459472656\n",
            "Epoch - 18, Loss - 556.755\n",
            "Hidden loss = 59.70648956298828, Reconstruction Loss = 52.00603103637695\n",
            "Epoch - 19, Loss - 596.591\n",
            "Hidden loss = 59.33680725097656, Reconstruction Loss = 53.666683197021484\n",
            "Epoch - 20, Loss - 549.146\n",
            "Hidden loss = 59.18981170654297, Reconstruction Loss = 49.82947540283203\n",
            "Epoch - 21, Loss - 563.981\n",
            "Hidden loss = 58.062583923339844, Reconstruction Loss = 50.67383575439453\n",
            "Epoch - 22, Loss - 557.239\n",
            "Hidden loss = 59.17085266113281, Reconstruction Loss = 50.761138916015625\n",
            "Epoch - 23, Loss - 549.08\n",
            "Hidden loss = 57.777034759521484, Reconstruction Loss = 52.077056884765625\n",
            "Epoch - 24, Loss - 558.542\n",
            "Hidden loss = 57.41259765625, Reconstruction Loss = 50.75297164916992\n",
            "Epoch - 25, Loss - 549.96\n",
            "Hidden loss = 57.61845779418945, Reconstruction Loss = 49.36149215698242\n",
            "Epoch - 26, Loss - 570.494\n",
            "Hidden loss = 55.25291061401367, Reconstruction Loss = 51.9340705871582\n",
            "Epoch - 27, Loss - 575.788\n",
            "Hidden loss = 55.361995697021484, Reconstruction Loss = 51.06052017211914\n",
            "Epoch - 28, Loss - 559.954\n",
            "Hidden loss = 55.194156646728516, Reconstruction Loss = 50.87617874145508\n",
            "Epoch - 29, Loss - 578.078\n",
            "Hidden loss = 52.87690734863281, Reconstruction Loss = 51.700191497802734\n",
            "Epoch - 30, Loss - 564.006\n",
            "Hidden loss = 52.78050994873047, Reconstruction Loss = 50.34336853027344\n",
            "Epoch - 31, Loss - 589.574\n",
            "Hidden loss = 54.30757522583008, Reconstruction Loss = 48.98931121826172\n",
            "Epoch - 32, Loss - 534.248\n",
            "Hidden loss = 53.39212417602539, Reconstruction Loss = 51.77227783203125\n",
            "Epoch - 33, Loss - 564.372\n",
            "Hidden loss = 52.86745834350586, Reconstruction Loss = 50.24627685546875\n",
            "Epoch - 34, Loss - 586.14\n",
            "Hidden loss = 52.007511138916016, Reconstruction Loss = 49.90705871582031\n",
            "Epoch - 35, Loss - 544.79\n",
            "Hidden loss = 54.31858825683594, Reconstruction Loss = 49.58055114746094\n",
            "Epoch - 36, Loss - 611.514\n",
            "Hidden loss = 49.85321044921875, Reconstruction Loss = 48.62776565551758\n",
            "Epoch - 37, Loss - 544.032\n",
            "Hidden loss = 52.915138244628906, Reconstruction Loss = 50.65296936035156\n",
            "Epoch - 38, Loss - 570.214\n",
            "Hidden loss = 49.613555908203125, Reconstruction Loss = 51.43461608886719\n",
            "Epoch - 39, Loss - 574.593\n",
            "Hidden loss = 52.36145782470703, Reconstruction Loss = 51.23365783691406\n",
            "Epoch - 40, Loss - 580.011\n",
            "Hidden loss = 51.10948181152344, Reconstruction Loss = 51.09479904174805\n",
            "Epoch - 41, Loss - 596.525\n",
            "Hidden loss = 50.24275207519531, Reconstruction Loss = 48.25160217285156\n",
            "Epoch - 42, Loss - 511.405\n",
            "Hidden loss = 48.91868209838867, Reconstruction Loss = 51.8858528137207\n",
            "Epoch - 43, Loss - 591.699\n",
            "Hidden loss = 50.86640930175781, Reconstruction Loss = 51.469482421875\n",
            "Epoch - 44, Loss - 643.886\n",
            "Hidden loss = 50.4696159362793, Reconstruction Loss = 51.70643615722656\n",
            "Epoch - 45, Loss - 553.961\n",
            "Hidden loss = 51.22964859008789, Reconstruction Loss = 50.485877990722656\n",
            "Epoch - 46, Loss - 545.603\n",
            "Hidden loss = 49.604591369628906, Reconstruction Loss = 50.097869873046875\n",
            "Epoch - 47, Loss - 590.899\n",
            "Hidden loss = 52.95266342163086, Reconstruction Loss = 49.37533187866211\n",
            "Epoch - 48, Loss - 573.069\n",
            "Hidden loss = 48.746185302734375, Reconstruction Loss = 53.374324798583984\n",
            "Epoch - 49, Loss - 547.881\n",
            "Hidden loss = 49.17927932739258, Reconstruction Loss = 51.95625305175781\n",
            "Epoch - 50, Loss - 543.789\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbbtS6Wcw8uv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E__fDRmZOY3F",
        "colab_type": "text"
      },
      "source": [
        "# Results for DCCA loss at hidden layer\n",
        "**Scores for Mediamill dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xyzXLD8xCL0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c723cc76-d2dd-42c6-fc88-53cd8df36302"
      },
      "source": [
        "print(\"P@1: \",p_k(y_pred, Y_test, 1))\n",
        "print(\"P@3: \",p_k(y_pred, Y_test, 3))\n",
        "print(\"P@5: \",p_k(y_pred, Y_test, 5))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "P@1:  0.7543750967941769\n",
            "P@3:  0.5022198131227416\n",
            "P@5:  0.31277683134584155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzrIuTT_xF3k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f073171c-6cf4-4640-f82b-955fb6814586"
      },
      "source": [
        "print(\"n@1: \",n_k(y_pred, Y_test, 1))\n",
        "print(\"n@3: \",n_k(y_pred, Y_test, 3))\n",
        "print(\"n@5: \",n_k(y_pred, Y_test, 5))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n@1:  0.7543750967941769\n",
            "n@3:  0.5895752551789218\n",
            "n@5:  0.49786608519459274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGQSwe2mOwz2",
        "colab_type": "text"
      },
      "source": [
        "# Grid Search for DCCA parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS2e_FKaxvtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for r in [ .5*1e-7, 1e-8, .5*1e-8, 1e-9]: \n",
        "    for f in [True, False]: \n",
        "        try: \n",
        "            model = XML(input_size=X_train.shape[1], embedding_size=100, attention_layer_size=50, \n",
        "                        encoder_layer_size=120, hidden_layer_size=100,output_size=Y_train.shape[1],\n",
        "                        margin=0.8, r1 = r, use_all_singular_values = f)\n",
        "            model.train(X_train, Y_train, lr_ = 0.001, epochs=20, batch_size=1024) \n",
        "            print(\"##############################################################################\") \n",
        "            print(\"Successfull Iteration for r = \", r, \" and f = \", f)\n",
        "        except : \n",
        "            print(\"##############################################################################\") \n",
        "            print(\"Error Iteration Aman for r = \", r, \" and f = \", f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVIk9BARKeeI",
        "colab_type": "text"
      },
      "source": [
        "# Results for MSE loss at hidden layer\n",
        "**Scores for Mediamill dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGOHwHbODfVp",
        "colab_type": "code",
        "outputId": "b174d537-149f-4101-823b-f7a220bfe461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(\"P@1: \",p_k(y_pred, Y_test, 1))\n",
        "print(\"P@3: \",p_k(y_pred, Y_test, 3))\n",
        "print(\"P@5: \",p_k(y_pred, Y_test, 5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "P@1:  0.6283103608486913\n",
            "P@3:  0.5773837179288812\n",
            "P@5:  0.4468483816013554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgPKMcZNAm0X",
        "colab_type": "code",
        "outputId": "5b100358-426c-4985-b03b-a4b748de664e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(\"n@1: \",n_k(y_pred, Y_test, 1))\n",
        "print(\"n@3: \",n_k(y_pred, Y_test, 3))\n",
        "print(\"n@5: \",n_k(y_pred, Y_test, 5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n@1:  0.6283103608486913\n",
            "n@3:  0.6284964713729472\n",
            "n@5:  0.6027628038215489\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}